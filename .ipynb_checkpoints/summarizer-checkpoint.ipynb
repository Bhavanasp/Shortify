{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "painted-tucson",
   "metadata": {},
   "source": [
    "## Summarization Model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focal-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "class sentence(object):\n",
    "\n",
    "    def __init__(self, docName, preproWords, originalWords):\n",
    "        self.docName = docName\n",
    "        self.preproWords = preproWords\n",
    "        self.wordFrequencies = self.sentenceWordFreq()\n",
    "        self.originalWords = originalWords\n",
    "\n",
    "    def getDocName(self):\n",
    "        return self.docName\n",
    "\n",
    "    def getPreProWords(self):\n",
    "        return self.preproWords\n",
    "\n",
    "    def getOriginalWords(self):\n",
    "        return self.originalWords\n",
    "\n",
    "    def getWordFreq(self):\n",
    "        return self.wordFrequencies\t\n",
    "\n",
    "    def sentenceWordFreq(self):\n",
    "        wordFreq = {}\n",
    "        for word in self.preproWords:\n",
    "            if word not in wordFreq.keys():\n",
    "                wordFreq[word] = 1\n",
    "            else:\n",
    "                wordFreq[word] = wordFreq[word] + 1\n",
    "        return wordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compliant-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getSentences(answer, id):\n",
    "\n",
    "    sentence_token = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    lines = sentence_token.tokenize(answer.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        originalWords = line[:]\n",
    "        line = line.strip().lower()\n",
    "\n",
    "        sent = nltk.word_tokenize(line)\n",
    "\n",
    "        stemmedSent = [porter.stem(word) for word in sent]\n",
    "        stemmedSent = filter(lambda x: x!='.'and x!='`'and x!=','and x!='?'and x!=\"'\" \n",
    "            and x!='!' and x!='''\"''' and x!=\"''\" and x!=\"'s\", stemmedSent)\n",
    "\n",
    "        if stemmedSent != []:\n",
    "            sentences.append(sentence(id, stemmedSent, originalWords))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-contract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "written-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFs(sentences):\n",
    "    \n",
    "    tfs = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        wordFreqs = sent.getWordFreq()\n",
    "\n",
    "        for word in wordFreqs.keys():\n",
    "            if tfs.get(word, 0) != 0:\n",
    "                tfs[word] = tfs[word] + wordFreqs[word]\n",
    "            else:\n",
    "                tfs[word] = wordFreqs[word]\t\n",
    "                \n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDFs(sentences):\n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    w2 = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        \n",
    "        for word in sent.getPreProWords():\n",
    "\n",
    "            if sent.getWordFreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+ 1\n",
    "\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "        \n",
    "        try:\n",
    "            w2.append(n)\n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "                \n",
    "        idfs[word] = idf\n",
    "            \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "progressive-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(sentences):\n",
    "    tfs = TFs(sentences)\n",
    "    idfs = IDFs(sentences)\n",
    "    retval = {}\n",
    "    for word in tfs:\n",
    "        tf_idfs=  tfs[word]\n",
    "        if retval.get(tf_idfs, None) == None:\n",
    "            retval[tf_idfs] = [word]\n",
    "        else:\n",
    "            retval[tf_idfs].append(word)\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tamil-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSim(sentence1, sentence2, IDF_w):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "\n",
    "    for word in sentence2.getPreProWords():\n",
    "        numerator+= sentence1.getWordFreq().get(word,0) * sentence2.getWordFreq().get(word,0) *  IDF_w.get(word,0) ** 2\n",
    "\n",
    "    for word in sentence1.getPreProWords():\n",
    "        denominator+= ( sentence1.getWordFreq().get(word,0) * IDF_w.get(word,0) ) ** 2\n",
    "\n",
    "    try:\n",
    "        return numerator / math.sqrt(denominator)\n",
    "    except ZeroDivisionError:\n",
    "        return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noticed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildQuery(sentences, TF_IDF_w, n):\n",
    "    \n",
    "    scores = TF_IDF_w.keys()\n",
    "    sorted(scores, reverse=True)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    queryWords = []\n",
    "\n",
    "    while(i<n):\n",
    "        words = TF_IDF_w[list(scores)[j]]\n",
    "        for word in words:\n",
    "            queryWords.append(word)\n",
    "            i=i+1\n",
    "            if (i>n): \n",
    "                break\n",
    "        j=j+1\n",
    "\n",
    "    return sentence(\"query\", queryWords, queryWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acoustic-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSentence(sentences, query, IDF):\n",
    "    best_sentence = None\n",
    "    maxVal = float(\"-inf\")\n",
    "\n",
    "    for sent in sentences:\n",
    "        similarity = sentenceSim(sent, query, IDF)\n",
    "\n",
    "        if similarity > maxVal:\n",
    "            best_sentence = sent\n",
    "            maxVal = similarity\n",
    "    if best_sentence in sentences:\n",
    "        sentences.remove(best_sentence)\n",
    "\n",
    "    return best_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verified-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):\n",
    "    summary = [best_sentence]\n",
    "    sum_len = len(best_sentence.getPreProWords())\n",
    "\n",
    "    MMRval={}\n",
    "\n",
    "    while (sum_len < summary_length):\n",
    "        MMRval={}\n",
    "\n",
    "        for sent in sentences:\n",
    "            MMRval[sent] = MMRScore(sent, query, summary, lambta, IDF)\n",
    "\n",
    "        maxxer = max(MMRval, key=MMRval.get)\n",
    "        summary.append(maxxer)\n",
    "        if maxxer in sentences:\n",
    "            sentences.remove(maxxer)\n",
    "        sum_len += len(maxxer.getPreProWords())\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weighted-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRScore(Si, query, Sj, lambta, IDF):\n",
    "    Sim1 = sentenceSim(Si, query, IDF)\n",
    "    l_expr = lambta * Sim1\n",
    "    value = [float(\"-inf\")]\n",
    "\n",
    "    for sent in Sj:\n",
    "        Sim2 = sentenceSim(Si, sent, IDF)\n",
    "        value.append(Sim2)\n",
    "\n",
    "    r_expr = (1-lambta) * max(value)\n",
    "    MMR_SCORE = l_expr - r_expr\t\n",
    "\n",
    "    return MMRScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "antique-february",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getPreProWords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9b1fc0c476c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mbest1sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDF_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest1sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDF_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mfinal_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0ba1682f1106>\u001b[0m in \u001b[0;36mmakeSummary\u001b[1;34m(sentences, best_sentence, query, summary_length, lambta, IDF)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmakeSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msum_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPreProWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mMMRval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getPreProWords'"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "answer = \"In 1992, Tim Berners-Lee circulated a document titled \\\"HTML Tags,\\\" which outlined just 20 tags, many of which are now obsolete or have taken other forms. The first surviving tag to be defined in the document, after the crucial anchor tag, is the paragraph tag. It wasn’t until 1993 that a discussion emerged on the proposed image tag.\"\n",
    "\n",
    "answers = [answer]\n",
    "\n",
    "for answer in answers:\n",
    "    sentences = sentences + getSentences(answer, 0)\n",
    "    \n",
    "IDF_w = IDFs(sentences)\n",
    "TF_IDF_w = TF_IDF(sentences)\n",
    "\n",
    "query = buildQuery(sentences, TF_IDF_w, 10)\n",
    "\n",
    "best1sentence = bestSentence(sentences, query, IDF_w)\n",
    "\n",
    "summary = makeSummary(sentences, best1sentence, query, 100, 0.5, IDF_w)\n",
    "\n",
    "final_summary = \"\"\n",
    "for sent in summary:\n",
    "    final_summary = final_summary + sent.getOriginalWords() + \"\\n\"\n",
    "final_summary = final_summary[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-mission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-question",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-glenn",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-membrane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "celtic-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "class sentence:\n",
    "    \n",
    "    def __init__(self, id, processedWords, originalWords):\n",
    "        self.id = id\n",
    "        self.processedWords = processedWords\n",
    "        self.originalWords = originalWords\n",
    "        self.wordFreq = self.sentWordFreq()\n",
    "\n",
    "    def getId(self):\n",
    "        return self.id\n",
    "\n",
    "    def getProcessedWords(self):\n",
    "        return self.processedWords\n",
    "\n",
    "    def getOriginalWords(self):\n",
    "        return self.originalWords\n",
    "    \n",
    "    def getWordFreq(self):\n",
    "        return self.wordFreq\n",
    "\n",
    "    def sentWordFreq(self):\n",
    "        wordFreq = {}\n",
    "        for word in self.processedWords:\n",
    "            if word not in wordFreq.keys():\n",
    "                wordFreq[word] = 1\n",
    "            else:\n",
    "                wordFreq[word] = wordFreq[word] + 1\n",
    "        return wordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "internal-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def processAns(id, answer):\n",
    "\n",
    "    sentence_token = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    lines = sentence_token.tokenize(answer.strip())\n",
    "\n",
    "    sentences = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    for line in lines:\n",
    "        originalWords = line[:]\n",
    "        line = line.strip().lower()\n",
    "\n",
    "        sent = nltk.word_tokenize(line)\n",
    "\n",
    "        stemmedSent = [re.sub(r'[^\\w\\s]', '', porter.stem(word)) for word in sent]\n",
    "\n",
    "        new_stemsent = []\n",
    "\n",
    "        for word in stemmedSent:\n",
    "            if word!='':\n",
    "                new_stemsent.append(word)\n",
    "\n",
    "        stemmedSent = new_stemsent\n",
    "        \n",
    "        if stemmedSent != [] :\n",
    "            sentences.append(sentence(id, stemmedSent, originalWords))\n",
    "\n",
    "    return sentences        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dependent-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFs(sentences):\n",
    "    \n",
    "    tfs = {}\n",
    "    \n",
    "    for sent in sentences:\n",
    "        wordFreqs = sent.getWordFreq()\n",
    "\n",
    "        for word in wordFreqs.keys():\n",
    "            if tfs.get(word, 0)!=0:\n",
    "                tfs[word] = tfs[word] + wordFreqs[word]\n",
    "            else:\n",
    "                tfs[word] = wordFreqs[word]\n",
    "        \n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "roman-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDFs(sentences):\n",
    "    \n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    w2 = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "\n",
    "        for word in sent.getProcessedWords():\n",
    "\n",
    "            if sent.getWordFreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+1\n",
    "\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "\n",
    "        try:\n",
    "            w2.append(n)                \n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "\n",
    "        idfs[word] = idf\n",
    "\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "hundred-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(sentences):\n",
    "    tfs = TFs(sentences)\n",
    "    idfs = IDFs(sentences)\n",
    "    retval = {}\n",
    "\n",
    "    for word in tfs:\n",
    "        tf_idfs = tfs[word]*idfs[word]\n",
    "\n",
    "        if retval.get(tf_idfs, None) == None:\n",
    "            retval[tf_idfs] = [word]\n",
    "        else:\n",
    "            retval[tf_idfs].append(word)\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "macro-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSim(sentence1, sentence2, IDF):\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "\n",
    "    for word in sentence2.getProcessedWords():\n",
    "        numerator+= sentence1.getWordFreq().get(word,0) * sentence2.getWordFreq().get(word,0) *  IDF.get(word,0) ** 2\n",
    "\n",
    "    for word in sentence1.getProcessedWords():\n",
    "        denominator+= ( sentence1.getWordFreq().get(word,0) * IDF.get(word,0) ) ** 2\n",
    "\n",
    "    try:\n",
    "        return numerator / math.sqrt(denominator)\n",
    "    except ZeroDivisionError:\n",
    "        return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "widespread-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBase(sentences, TF_IDF_dict, n):\n",
    "\n",
    "    scores = TF_IDF_dict.keys()\n",
    "    sorted(scores, reverse=True)\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    baseWords = []\n",
    "\n",
    "    while(i<n):\n",
    "        words = TF_IDF_dict[list(scores)[j]]\n",
    "        for word in words:\n",
    "            baseWords.append(word)\n",
    "            i = i+1\n",
    "            if(i>n):\n",
    "                break\n",
    "        j = j+1\n",
    "\n",
    "        return sentence(\"base\", baseWords, baseWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "prescription-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSentence(senteces, base, IDF):\n",
    "    best_sentence = None\n",
    "    maxVal = float(\"-inf\")\n",
    "\n",
    "    for sent in sentences:\n",
    "        similarity = sentenceSim(sent, base, IDF)\n",
    "\n",
    "        if similarity > maxVal:\n",
    "            best_sentence = sent\n",
    "            maxVal = similarity\n",
    "\n",
    "    if best_sentence != None:\n",
    "        sentences.remove(best_sentence)\n",
    "\n",
    "    return best_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "approved-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRScore(Si, base, Sj, lambta, IDF):\n",
    "    Sim1 = sentenceSim(Si, base, IDF)\n",
    "    l_expr = lambta * Sim1\n",
    "    value = [float(\"-inf\")]\n",
    "\n",
    "    for sent in Sj:\n",
    "        Sim2 = sentenceSim(Si, sent, IDF)\n",
    "        value.append(Sim2)\n",
    "\n",
    "    r_expr = (1-lambta)*max(value)\n",
    "    MMR_SCORE = l_expr-r_expr\n",
    "\n",
    "    return MMRScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSummary(sentences, best_sentence, base, summary_len, lambta, IDF):\n",
    "    summary = [best_sentence]\n",
    "    sum_len = len(best_sentence.getProcessedWords())\n",
    "\n",
    "    MMRval = {}\n",
    "\n",
    "    while(sum_len<summary_len):\n",
    "        MMRval={}\n",
    "\n",
    "        for sent in sentences:\n",
    "            MMRval[sent] = MMRScore(sent, base, summary, lambta, IDF)\n",
    "\n",
    "        maxxer = max(MMRval, key=MMRval.get)\n",
    "        if maxxer != None:\n",
    "            summary.append(maxxer)\n",
    "            sentences.remove(maxxer)\n",
    "            sum_len += len(maxxer.getPreProWords())\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"In this article, I’d like to reacquaint you with the humble workhorse of communication that is the paragraph. Paragraphs are everywhere. In fact, at the high risk of stating the obvious, you are reading one now. Despite their ubiquity, we frequently neglect their presentation. This is a mistake.\"\n",
    "\n",
    "answers = [answer]\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for answer in answers:\n",
    "    sentences = sentences + processAns(\"0\", answer)\n",
    "    \n",
    "IDF = IDFs(sentences)\n",
    "TF_IDF_dict = TF_IDF(sentences)\n",
    "\n",
    "base = buildBase(sentences, TF_IDF_dict, 10)\n",
    "\n",
    "bestsent = bestSentence(sentences, base, IDF)\n",
    "\n",
    "summary = makeSummary(sentences, bestsent, base, 100, 0.5, IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-blackberry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-military",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-custom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-genius",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
