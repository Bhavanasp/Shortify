id,answer,link,code,sim score,sc
43948049.0,"There are many ways to do this, staying in Pandas I did the following.With the file structureThis code will work, it's a little verbose for explanation but you can shorten with implementation.",,"root/  
├── dir1/  
│   ├── data_20170101_k   
│   ├── data_20170102_k    
│   ├── ...  
├── dir2/    
│   ├── data_20170101_k    
│   └── data_20170101_k  
│   └── ...   
└── ... 

import glob
import pandas as pd

CONCAT_DIR = ""/FILES_CONCAT/""

# Use glob module to return all csv files under root directory. Create DF from this.
files = pd.DataFrame([file for file in glob.glob(""root/*/*"")], columns=[""fullpath""])

#    fullpath
# 0  root\dir1\data_20170101_k.csv
# 1  root\dir1\data_20170102_k.csv
# 2  root\dir2\data_20170101_k.csv
# 3  root\dir2\data_20170102_k.csv

# Split the full path into directory and filename
files_split = files['fullpath'].str.rsplit(""\\"", 1, expand=True).rename(columns={0: 'path', 1:'filename'})

#    path       filename
# 0  root\dir1  data_20170101_k.csv
# 1  root\dir1  data_20170102_k.csv
# 2  root\dir2  data_20170101_k.csv
# 3  root\dir2  data_20170102_k.csv

# Join these into one DataFrame
files = files.join(files_split)

#    fullpath                       path        filename
# 0  root\dir1\data_20170101_k.csv  root\dir1   data_20170101_k.csv
# 1  root\dir1\data_20170102_k.csv  root\dir1   data_20170102_k.csv
# 2  root\dir2\data_20170101_k.csv  root\dir2   data_20170101_k.csv
# 3  root\dir2\data_20170102_k.csv  root\dir2   data_20170102_k.csv

# Iterate over unique filenames; read CSVs, concat DFs, save file
for f in files['filename'].unique():
    paths = files[files['filename'] == f]['fullpath'] # Get list of fullpaths from unique filenames
    dfs = [pd.read_csv(path, header=None) for path in paths] # Get list of dataframes from CSV file paths
    concat_df = pd.concat(dfs) # Concat dataframes into one
    concat_df.to_csv(CONCAT_DIR + f) # Save dataframe
",1.0,3.988705036026142
43552785.0,Consider a concatenation with merge which would translate your SQL query as OR is often analogous to a UNION:,,"OR
UNION
pd.concat([pd.merge(table_A, table_B, on='one'),
           pd.merge(table_A, table_B, left_on='two', right_on='one')])
",0.95,3.896194785390617
22306340.0,"Is there a reason you're avoiding creating some objects to manage this? If it were me, I'd go objects and do something like the following (this is completely untested, there may be typos):Now I can right code that looks like:This is just a starting point. Now you can you add methods that group (and sum) the expenditures list by decoration (e.g. ""I spent HOW MUCH on Twinkies last month???""). You can add a method that parses entries from a file, or emits them to a csv list. You can do some charting based on time.",,"#!/usr/bin/env python3

from datetime import datetime # why python guys, do you make me write code like this??
from operator import itemgetter

class BudgetCategory(object):
    def __init__(self, name, allowance):
        super().__init__()
            self.name = name # string naming this category, e.g. 'Food'
            self.allowance = allowance # e.g. 400.00 this month for Food
            self.expenditures = [] # initially empty list of expenditures you've made

    def spend(self, amount, when=None, description=None):
        ''' Use this to add expenditures to your budget category'''
        timeOfExpenditure = datetime.utcnow() if when is None else when #optional argument for time of expenditure
        record = (amount, timeOfExpenditure, '' if description is None else description) # a named tuple would be better here...
        self.expenditures.append(record) # add to list of expenditures
        self.expenditures.sort(key=itemgetter(1)) # keep them sorted by date for the fun of it

    # Very tempting to the turn both of the following into @property decorated functions, but let's swallow only so much today, huh?
    def totalSpent(self):
        return sum(t[0] for t in self.expenditures)

    def balance(self):
        return self.allowance - self.totalSpent()

budget = BudgetCategory(name='Food', allowance=200)
budget.spend(5)
budget.spend(8)

print('total spent:', budget.totalSpent())
print('left to go:', budget.balance())
",1.0,3.491719570830387
57041468.0,"you can use groupby to get all the matching dicts, then unify them using ChainMap, like this:Output:",https://docs.python.org/3/library/itertools.html#itertools.groupby https://docs.python.org/3/library/collections.html#collections.ChainMap,"groupby
ChainMap
from itertools import groupby
from operator import itemgetter
from collections import ChainMap

list1 = [{'name': 'Nick', 'id': '123456'}, {'name': 'Donald', 'id': '999'}]
list2 = [{'address': 'London', 'id': '123456'}, {'address': 'NYC', 'id': '999'}]

grouped_subdicts = groupby(sorted(list1 + list2, key=itemgetter(""id"")), itemgetter(""id""))

result = [dict(ChainMap(*g)) for k, g in grouped_subdicts]

print(result)

[{'id': '123456', 'address': 'London', 'name': 'Nick'},
{'id': '999', 'address': 'NYC', 'name': 'Donald'}]
",0.95,3.4518714390299623
58276538.0,"This is perfect case for DataFrame.update, which aligns on indicesOutputNote that update is in place, as quoted from the documentation:Modify in place using non-NA values from another DataFrame.That means that your original dataframe will be updated by the new values. To prevent this, use:",https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html,"DataFrame.update
Empty_DF.update(ROI_DF)

print(df3)

     a    b  c
a  0.0  5.0  0
b  1.0  6.0  0
c  2.0  7.0  0
d  0.0  0.0  0
e  3.0  8.0  0
f  0.0  0.0  0

update
df3 = Empty_DF.copy()
df3.update(ROI_DF)
",0.95,3.3608160166910204
42265239.0,"Get all the keys using a set union d1.keys() | d2.keys(), then it's a dict comprehension:Note: If you get TypeError then you're using an older version of Python.  You can just change .keys() to .viewkeys() to make it work in that case.  ",,"d1.keys() | d2.keys()
>>> {k: d1.get(k, []) + d2.get(k, []) for k in (d1.keys() | d2.keys())}
{'1': [12, 32, 44, 12, 34, 15, 11, 44, 42, 14],
 '2': [21, 34, 11, 65, 11, 24, 41, 65],
 '3': [44, 12, 98, 41, 22, 48],
 '4': [65, 71]}

TypeError
.keys()
.viewkeys()",1.0,3.3605554051503526
20278996.0,If dicts have same keys:if not:and you can use collections.defaultdict in this solution:,,">>> {k: v + dict2.get(k, []) for k, v in dict1.iteritems()}
{'a': [1, 2, 3, 100, 101, 103], 'b': [4, 5, 6, 104, 105, 106]}

>>> from itertools import chain
>>> res = {}
>>> for k, v in chain(dict1.iteritems(), dict2.iteritems()):
...     res[k] = res.get(k, []) + v
... 
>>> res
{'a': [1, 2, 3, 100, 101, 103], 'b': [4, 5, 6, 104, 105, 106]}

collections.defaultdict
>>> from collections import defaultdict
>>> res = defaultdict(list)
>>> for k, v in chain(dict1.iteritems(), dict2.iteritems()):
...     res[k] += v
... 
>>> dict(res)
{'a': [1, 2, 3, 100, 101, 103], 'b': [4, 5, 6, 104, 105, 106]}
",0.95,3.3598659438620047
52879192.0,"I suggest you to use json, which is specific for JSON object manipulation. You can do something like this:where json.load() convert a string to a json object, while json.dumps() convert a json to a string. The parameter indent let you print the object in the expanded way.",,"json
    import json

with open('example1.json') as f:
    data1 = json.load(f)

with open('example2.json') as f:
    data2 = json.load(f)

with open('example3.json') as f:
    data3 = json.load(f)

items1 = data1[""items""]
#print(json.dumps(items1, indent=2))
items2 = data2[""items""]
items3 = data3[""items""]

listitem = [items1, items2, items3]
finaljson = {""items"" : []}

finaljson[""items""].append(items1)
finaljson[""items""].append(items2)
finaljson[""items""].append(items3)
print(json.dumps(finaljson, indent=2))

with open('merged_json.json', ""w"") as f:
    f.write(json.dumps(finaljson, indent=2))

json.load()
json.dumps()
indent",1.0,3.3501913348357815
55206146.0,you can just assign an array as a column:,,"a = ['g', 'h', 'y']
df['array']=a
print(df)

   index col1 col2 array
0      1    a    b     g
1      1    r    t     h
2      2    e    e     y
",0.95,3.3329990516760084
54591008.0,sql equivalent with where:that would translate to python as:,,"SELECT t1.company,
        t1.resource,
        t2.company,
        t2.resource,
        t1.ClockInDate,
        t2.EffectiveFrom,
        t2.EffectiveTo
FROM table1 t1
LEFT JOIN table2 t2 ON t1.resource = t2.resource
                    AND t1.company = t2.company
WHERE t1.ClockInDate IS NULL --no ClockInDate to check
    OR t2.company IS NULL AND t2.resource IS NULL --not rows in t2 for t1
    OR t1.ClockInDate BETWEEN t2.EffectiveFrom AND t2.EffectiveTo --ClockInDate exists, rows in t2 exist, we can now check ClockInDate to be between t2.EffectiveFrom AND t2.EffectiveTo

df_merge = pd.merge(df1, df2, on=['Company', 'Resource'], how='left')
df_final = df_merge[df_merge.ClockInDate.isnull() | df_merge.ClockInDate.between(df_merge.EffectiveFrom, df_merge.EffectiveTo) | df_merge.EffectiveFrom.isnull()]
",1.0,3.255316123641922
44568806.0,"temp (temperature) dataframe:humid dataframe:Now the temp dataframe looks like:Do the same for humid df:Now humid is:Reindex temp, replace the dates as you like:And now left join:Make sure this actually worked:Hooray!",,"                 datetime  temperature
0  2017-06-13 22:20:11.309         82.4
1  2017-06-13 22:19:54.004         82.4
2  2017-06-13 22:19:36.661         82.4
3  2017-06-13 22:19:19.359         82.4

                 datetime  humidity
0  2017-06-13 22:07:30.723      63.0
1  2017-06-13 22:07:13.448      63.0
2  2017-06-13 22:06:56.115      63.0
3  2017-06-13 22:06:38.806      63.0



temp.datetime = pd.to_datetime(temp.datetime) #convert to datetime dtype
temp.set_index('datetime', inplace=True) #make it the index
temp.index = temp.index.round('S') #and now round to the second

                     temperature
datetime                        
2017-06-13 22:20:11         82.4
2017-06-13 22:19:54         82.4
2017-06-13 22:19:37         82.4
2017-06-13 22:19:19         82.4

humid.datetime = pd.to_datetime(humid.datetime) 
humi.set_index('datetime', inplace=True) 
humid.index = humid.index.round('S') 

                     humidity
datetime                     
2017-06-13 22:07:31      63.0
2017-06-13 22:07:13      63.0
2017-06-13 22:06:56      63.0
2017-06-13 22:06:39      63.0

temp = temp.reindex(pd.DatetimeIndex(start='2017-06-13 22:00', end='2017-06-13 22:20', freq='S'))
temp.head()

                     temperature
2017-06-13 22:00:00          NaN
2017-06-13 22:00:01          NaN
2017-06-13 22:00:02          NaN
2017-06-13 22:00:03          NaN
2017-06-13 22:00:04          NaN

out = pd.merge(temp, humid, left_index=True, right_index=True, how='left')

out.head():
                     temperature  humidity
2017-06-13 22:00:00          NaN       NaN
2017-06-13 22:00:01          NaN       NaN
2017-06-13 22:00:02          NaN       NaN
2017-06-13 22:00:03          NaN       NaN
2017-06-13 22:00:04          NaN       NaN

out.loc['2017-06-13 22:07:31']
                     temperature  humidity
2017-06-13 22:07:31          NaN      63.0
",1.0,3.1808575516310875
59522083.0,"When you create a variable in a method, it is a local variable by default, and is not accessible outside that method. Your lines:andcreate local variables named metropolitalne_text. I suspect you really want metropolitalne_text to be a property of MainScreen. You can do this by adding a line to your definition of the MainScreen class:Then, everywhare that you reference metropolitalne_text in the MainScreen class, change it to self.metropolitalne_text.",,"metropolitalne_text=StringProperty('Bilety metropolitalne')

metropolitalne_text=StringProperty('Metropolitan tickets')

metropolitalne_text
metropolitalne_text
MainScreen
MainScreen
class MainScreen (Screen):
    metropolitalne_text = StringProperty()

    def __init__(self,**kwargs):
        super().__init__()

        self.button_polish = Button(on_press=self.change_language_to_polish)
        self.button_english = Button(on_press=self.change_language_to_english)

metropolitalne_text
MainScreen
self.metropolitalne_text",1.0,3.176801008640094
43399506.0,"This isn't pretty, but what if you did something like this:And this would be the general case:",,"df2 = DataFrame(df, copy=True)
df2[['lat2', 'lon2']] = df[['lat', 'lon']].shift(-1)
df2.set_index(['lat', 'lon', 'lat2', 'lon2'], inplace=True)
print(df2.loc[(12, 10, 13, 9)].reset_index(drop=True))

   car_id
0     100
1     120

raw_data = {'car_id': [100, 100, 100, 110, 110, 110, 110, 120, 120, 120, 120, 130],
            'lat': [10, 12, 13, 23, 13, 12, 12, 11, 12, 13, 14, 12],
            'lon': [15, 10, 9, 8, 9, 10, 2, 11, 10, 9, 8, 10],
           }
df = pd.DataFrame(raw_data, columns = ['car_id', 'lat', 'lon'])

raw_data = {
             'lat': [10, 12, 13],
             'lon': [15, 10, 9],
           }

coords = pd.DataFrame(raw_data, columns = ['lat', 'lon'])

def submatch(df, match):
    df2 = DataFrame(df['car_id'])
    for x in range(match.shape[0]):
        df2[['lat{}'.format(x), 'lon{}'.format(x)]] = df[['lat', 'lon']].shift(-x)

    n = match.shape[0]
    cols = [item for sublist in
        [['lat{}'.format(x), 'lon{}'.format(x)] for x in range(n)]
        for item in sublist]

    df2.set_index(cols, inplace=True)
    return df2.loc[tuple(match.stack().values)].reset_index(drop=True)

print(submatch(df, coords))

   car_id
0     100
",0.95,3.1097027272772504
48641350.0,"Or you can use use append, but you need to create new copy of the lst1:",,"lst3 = []
for elem in lst2:
    theNew = lst1[:]
    theNew.append(new2)
    lst3.append(theNew)
print(lst3)
",0.9,2.977919775987422
52480347.0,"You can use pd.concat to literally join by the index of the dataframe.  This means both of your dataframes have to be preordered and you simply ""pasting"" one dataframe next to the other.Output:",,"pd.concat
pd.concat([df1, df2[['Issue']], axis=1)

  IDs  Value1  Value2 Issue
0  AB       1       3    AA
1  AB       1       1   AAA
2  AB       2       4    BA
3  BC       2       2    CC
4  BC       5       0    CA
5  BG       1       1     A
6  RF       2       2     D
",0.85,2.954198458752523
