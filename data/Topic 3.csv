answer,link,code,sc
"pyvips can do exactly what you want very quickly and efficiently. For example:The access=""sequential"" option tells pyvips that you want to stream the image. It will only load pixels on demand as it generates output, so you can merge enormous images using only a little memory. The arrayjoin operator joins an array of images into a grid across tiles across. It has quite a few layout options: you can specify borders, overlaps, background, centring behaviour and so on.I can run it like this:So it joined 100 JPG images to make a 14,000 x 20,000 pixel mosaic in about 2.5s on this laptop, and from watching top, needed about 300mb of memory. I've used it to join over 30,000 images into a single file, and it would go higher. I've made images of over 300,000 by 300,000 pixels.The pyvips equivalent of PIL's paste is insert. You could use that too, though it won't work so well for very large numbers of images. There's also a command-line interface, so you could just enter:To join up a large set of JPG images.",https://pypi.org/project/pyvips/ https://libvips.github.io/libvips/API/current/libvips-conversion.html#vips-arrayjoin https://libvips.github.io/libvips/API/current/libvips-conversion.html#vips-insert,"import sys
import pyvips

images = [pyvips.Image.new_from_file(filename, access=""sequential"")
          for filename in sys.argv[2:]]
final = pyvips.Image.arrayjoin(images, across=10)
final.write_to_file(sys.argv[1])

access=""sequential""
arrayjoin
across
$ for i in {1..100}; do cp ~/pics/k2.jpg $i.jpg; done
$ time ../arrayjoin.py x.tif *.jpg 

real    0m2.498s
user    0m3.579s
sys 0m1.054s
$ vipsheader x.tif
x.tif: 14500x20480 uchar, 3 bands, srgb, tiffload

top
paste
insert
vips arrayjoin ""${echo *.jpg}"" x.tif --across 10
",2.334166017840742
"First of all, I assumed you've not repeated your strings correctly (like ""hi, this is an example line."" != ""hi, this is edited line."") by mistake, not on purpose (that I can't figure out).I named the accumulative file common.doc to distinct from the other .txt files in the target directory. Also, this example code implies all the files are in the same directory.And after common.doc editing:And a solution for multiline text (merging stays with .strip() removed on content writing), not suitable for hundreds of thousands of files tho...",,"common.doc
.txt
# merging.py
import os
import glob

with open(""common.doc"", ""w"") as common:
    for txt in glob.glob(""./*.txt""):
        with open(txt, ""r"") as f:
            content = f.read()
        common.write(""{} ({})\n"".format(os.path.basename(txt), content))

common.doc
# splitting.py
with open(""common.doc"", ""r"") as common:
    for line in common:
        name = line[:line.find("" ("")]
        text = line[line.find("" ("")+2:line.rfind("")"")]
        with open(name, ""w"") as f:
            f.write(text)

.strip()
# splitting2.py
with open(""common.doc"", ""r"") as common:
    everything = common.read()
elements = everything.split("")"")
for elem in elements:
    name = elem[:elem.find("" ("")].strip()
    text = elem[elem.find("" ("")+2:]
    if name:
        with open(name, ""w"") as f:
            f.write(text)
",1.6877263605956307
"Using pandas:To get rid of duplicate rows as well:This will not get rid of duplicates as the dataframe is created, but after. So a dataframe gets created by concatenating all of the files. Then it is de-duplicated. The final dataframe can then be saved to csv.",,"import pandas as pd

interesting_files = glob.glob(""/home/tcs/PYTHONMAP/test1/*.csv"") 
df = pd.concat((pd.read_csv(f, header = 0) for f in interesting_files))
df.to_csv(""output.csv"")

import pandas as pd

interesting_files = glob.glob(""/home/tcs/PYTHONMAP/test1/*.csv"") 
df = pd.concat((pd.read_csv(f, header = 0) for f in interesting_files))
df_deduplicated = df.drop_duplicates()
df_deduplicated.to_csv(""output.csv"")
",1.626503306616306
"With python's csv reader and writer,
reading from the files a.csv and b.csv, writing to c.csv:Creates file c.csv:(I assume that those braces and extra linebreaks in your csv code are just here on stackoverflow and not really part of your csv files? I also assume that a.csv and b.csv have the same length in lines.)",,"a.csv
b.csv
c.csv
# -*- coding: utf-8 -*-

import csv

with open('a.csv', 'r') as file_a:
    with open('b.csv', 'r') as file_b:
        with open('c.csv', 'w') as file_c:
            reader_a = csv.reader(file_a, delimiter=',')
            reader_b = csv.reader(file_b, delimiter=',')
            writer_c = csv.writer(file_c)

            for cols_a in reader_a:
                cols_b = reader_b.next()
                writer_c.writerow(cols_a + cols_b)

c.csv
a, b, c, d,o, p, q, r
e, f, g, h,s, t, u, v
i, j, k, l,w, x, y, z
",1.2960513101501008
"You can use merge with left join, if only X is joined column on parameter can be omit:If multiple same columns names:",http://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html,"merge
X
on
df = pd.merge(df1, df2, how='left')
print (df)
    A    B      X   C    D
0  NA   50  IW233  12  Jan
1  EU   23  IW455  50  Aug
2  LA   21  IW455  50  Aug
3  ME  100  IW100  16  Nov

df = pd.merge(df1, df2, on='X', how='left')
print (df)
    A    B      X   C    D
0  NA   50  IW233  12  Jan
1  EU   23  IW455  50  Aug
2  LA   21  IW455  50  Aug
3  ME  100  IW100  16  Nov
",1.2551234132500633
"Your csv file apparently has 5 columns, but your data is a single list of values. That means that you also only need 1 column header. Pandas complains right now because the dimension of the column list (5) does not match the number of columns in your data (1). You could fix this for example by saying:That is assuming that you want to use the first column name.",,"df = pd.DataFrame(data=data, index=None, columns=[columns[0]])
",1.1159742898708507
I would suggest to read the csv files into dataframes and concatenate them this way,,"frames = [pd.read_csv('f1.csv'), pd.read_csv('f2.csv')]
result = concat(frames,ignore_index=True)
",1.06685191285156
"This 'algorithm' tries to makes sense of the input without relying on line endings, so that it should work correctly with some input likeThe code lends itself to being integrated into a state machine - the loop only remembers its current phrase and ""pushes"" finished phrases off onto a list, and gobbles one word at a time. Splitting on whitespaces is good.Notice the ambiguity in case #5: that cannot be reliably solved (and it is possible to have such an ambiguity also with line endings. Maybe combining both...)Output:",,"born in the U.
S.A.

# Sample decoded data
decoded = [ 'Some', 'sentence', 'that', 'is', 'spliced.', '.', '.',
    'and', 'has', 'a', 'continuation.',
    'this', 'cannot', 'be', 'confused', 'by', 'U.', 'S.', 'A.', 'or', 'U.S.A.',
    'In', 'that', 'last', 'sentence...',
    'an', 'abbreviation', 'ended', 'the', 'sentence!' ]

# List of phrases
phrases = []

# Current phrase
phrase    = ''

while decoded:
    word = decoded.pop(0)
    # Possibilities:
    # 1. phrase has no terminator. Then we surely add word to phrase.
    if not phrase[-1:] in ('.', '?', '!'):
        phrase += ('' if '' == phrase else ' ') + word
        continue
    # 2. There was a terminator. Which?
    #    Say phrase is dot-terminated...
    if '.' == phrase[-1:]:
        # BUT it is terminated by several dots.
        if '..' == phrase[-2:]:
            if '.' == word:
                phrase += '.'
            else:
                phrase += ' ' + word
            continue
        # ...and word is dot-terminated. ""by U."" and ""S."", or ""the."" and ""."".
        if '.' == word[-1:]:
            phrase += word
            continue
        # Do we have an abbreviation?
        if len(phrase) > 3:
            if '.' == phrase[-3:-2]:
                # 5. We have an ambiguity, we solve using capitals.
                if word[:1].upper() == word[:1]:
                    phrases.append(phrase)
                    phrase = word
                    continue
                phrase += ' ' + word
                continue
        # Something else. Then phrase is completed and restarted.
        phrases.append(phrase)
        phrase = word
        continue
    # 3. Another terminator.
        phrases.append(phrase)
        phrase = word
        continue

phrases.append(phrase)

for p in phrases:
    print "">> "" + p

>> Some sentence that is spliced... and has a continuation.
>> this cannot be confused by U.S.A. or U.S.A.
>> In that last sentence... an abbreviation ended the sentence!
",1.0277856189100438
"This is perfect case for DataFrame.update, which aligns on indicesOutputNote that update is in place, as quoted from the documentation:Modify in place using non-NA values from another DataFrame.That means that your original dataframe will be updated by the new values. To prevent this, use:",https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html,"DataFrame.update
Empty_DF.update(ROI_DF)

print(df3)

     a    b  c
a  0.0  5.0  0
b  1.0  6.0  0
c  2.0  7.0  0
d  0.0  0.0  0
e  3.0  8.0  0
f  0.0  0.0  0

update
df3 = Empty_DF.copy()
df3.update(ROI_DF)
",1.008417036435381
"i['meds'] is not a key of description_collapses. The rules are the value of description_collapsed.get(i[""category""]), so you need to extend that value.You also had your parentheses wrong.Note that your final result will be a dictionary, not a list.",,"i['meds']
description_collapses
description_collapsed.get(i[""category""])
description_collapsed = {}
for i in description:
    if i[""category""] in description_collapsed:
        description_collapsed.[i[""category""]].extend(i[""meds""])
    else:
        description_collapsed[i[""category""]] = i[""meds""]
",0.9549607829803591
"You didn't show linar_eq, but I can guess it is: list() applied to a string breaks it up, producing a list of characters.np.array on that list doesn't change things, and won't make grouping the characters any easier.  Stick with list operations.The two proposed answers follow different 'rules'.  One looks like it takes the characters 2 by 2, but then drops the '='.  The other drops the 'x'.  Why?Your problem is not clearly specified.  But I suspect you'd be better off using the regex mechanism to split up the original string.  It isn't a merging problem.  It's more of a parsing one.",,"linar_eq
In [9]: eq = ""2x-4=5""

list()
In [10]: list(eq)
Out[10]: ['2', 'x', '-', '4', '=', '5']

np.array
['2x', '-4', '5']
['2', '-4', '5'] 

regex",0.9260055221515978
"You're using the csv module, but ignoring csv.reader.  It handles all the parsing for you:Input:Output in the .csv file:EDIT: I see now that you want the data in Excel to be in a single cell.  The above will put it in a row of four cells:The following will write a single cell:Output in the .csv file:Output in Excel:",https://i.stack.imgur.com/gVpaq.png https://i.stack.imgur.com/vi59a.png,"csv
csv.reader
#!python2
import csv
with open('book2.csv','rb') as inf, open('test.csv','wb') as outf:
    r = csv.reader(inf)
    w = csv.writer(outf)
    L = [row for row in r] # Read all lines as list of lists.
    L = zip(*L)            # transpose all the lines.
    w.writerows(L)         # Write them all back out.

222
333
444
555

222,333,444,555

#!python2
import csv
with open('book2.csv') as inf, open('test.csv','wb') as outf:
    w = csv.writer(outf)
    data = inf.read().splitlines()
    w.writerow([','.join(data)])

""222,333,444,555""
",0.87236575793284
"You can use pd.concat to literally join by the index of the dataframe.  This means both of your dataframes have to be preordered and you simply ""pasting"" one dataframe next to the other.Output:",,"pd.concat
pd.concat([df1, df2[['Issue']], axis=1)

  IDs  Value1  Value2 Issue
0  AB       1       3    AA
1  AB       1       1   AAA
2  AB       2       4    BA
3  BC       2       2    CC
4  BC       5       0    CA
5  BG       1       1     A
6  RF       2       2     D
",0.8624194000229738
If you're looking to get two colours on the same line you can use several labels and use .grid() to get them on the same line.If you know you wanted two words and two colours for example you can use something like this:Or if you wanted to have a different colours for each word in a string for example:,,".grid()
root = Tk()
Label(root,text=""red text"",fg=""red"").grid(column=0,row=0)
Label(root,text=""green text"",fg=""green"").grid(column=0,row=1)
mainloop()

words = [""word1"",""word2"",""word3"",""word4""]
colours = [""blue"",""green"",""red"",""yellow""]

for index,word in enumerate(words):
    Label(window,text = word,fg=colours[index]).grid(column=index,row=0)
",0.7106957769218512
"I suggest you to use json, which is specific for JSON object manipulation. You can do something like this:where json.load() convert a string to a json object, while json.dumps() convert a json to a string. The parameter indent let you print the object in the expanded way.",,"json
    import json

with open('example1.json') as f:
    data1 = json.load(f)

with open('example2.json') as f:
    data2 = json.load(f)

with open('example3.json') as f:
    data3 = json.load(f)

items1 = data1[""items""]
#print(json.dumps(items1, indent=2))
items2 = data2[""items""]
items3 = data3[""items""]

listitem = [items1, items2, items3]
finaljson = {""items"" : []}

finaljson[""items""].append(items1)
finaljson[""items""].append(items2)
finaljson[""items""].append(items3)
print(json.dumps(finaljson, indent=2))

with open('merged_json.json', ""w"") as f:
    f.write(json.dumps(finaljson, indent=2))

json.load()
json.dumps()
indent",0.7083230246804454
