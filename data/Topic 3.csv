answer,link,code,sc
"This is perfect case for DataFrame.update, which aligns on indicesOutputNote that update is in place, as quoted from the documentation:Modify in place using non-NA values from another DataFrame.That means that your original dataframe will be updated by the new values. To prevent this, use:",https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html,"DataFrame.update
Empty_DF.update(ROI_DF)

print(df3)

     a    b  c
a  0.0  5.0  0
b  1.0  6.0  0
c  2.0  7.0  0
d  0.0  0.0  0
e  3.0  8.0  0
f  0.0  0.0  0

update
df3 = Empty_DF.copy()
df3.update(ROI_DF)
",
sql equivalent with where:that would translate to python as:,,"SELECT t1.company,
        t1.resource,
        t2.company,
        t2.resource,
        t1.ClockInDate,
        t2.EffectiveFrom,
        t2.EffectiveTo
FROM table1 t1
LEFT JOIN table2 t2 ON t1.resource = t2.resource
                    AND t1.company = t2.company
WHERE t1.ClockInDate IS NULL --no ClockInDate to check
    OR t2.company IS NULL AND t2.resource IS NULL --not rows in t2 for t1
    OR t1.ClockInDate BETWEEN t2.EffectiveFrom AND t2.EffectiveTo --ClockInDate exists, rows in t2 exist, we can now check ClockInDate to be between t2.EffectiveFrom AND t2.EffectiveTo

df_merge = pd.merge(df1, df2, on=['Company', 'Resource'], how='left')
df_final = df_merge[df_merge.ClockInDate.isnull() | df_merge.ClockInDate.between(df_merge.EffectiveFrom, df_merge.EffectiveTo) | df_merge.EffectiveFrom.isnull()]
",
"This can be achieved in much simple way in shell as:(Note: Don't use .csv in extension as it will cause inconsistency with find. After this command is finished, file can be renamed as .csv",,"find . -name ""*.csv"" | xargs cat > mergedCSV
",
If you're looking to get two colours on the same line you can use several labels and use .grid() to get them on the same line.If you know you wanted two words and two colours for example you can use something like this:Or if you wanted to have a different colours for each word in a string for example:,,".grid()
root = Tk()
Label(root,text=""red text"",fg=""red"").grid(column=0,row=0)
Label(root,text=""green text"",fg=""green"").grid(column=0,row=1)
mainloop()

words = [""word1"",""word2"",""word3"",""word4""]
colours = [""blue"",""green"",""red"",""yellow""]

for index,word in enumerate(words):
    Label(window,text = word,fg=colours[index]).grid(column=index,row=0)
",
One more approachOutput,,"df1.join(df2['date'],rsuffix='df2',how='outer').join(df3['date'],rsuffix='df3',how='outer')

  Color     date        datedf2     datedf3
0   A       2011.0      2013.0      2011
1   B       201411.0    20151111.0  201411
2   C       20151231.0  201101.0    20151231
3   A       2019.0      NaN         2019
4   NaN     NaN         NaN         20070212
",
"You can use DataFrame.join with rename and parameter on, then DataFrame.set_index with DataFrame.reorder_levels:Or use Index.map:",http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reorder_levels.html http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.map.html,"DataFrame.join
rename
on
DataFrame.set_index
DataFrame.reorder_levels
result = (df.join(s1.rename('cnts'), on='Pet')
           .set_index('cnts', append=True)
           .reorder_levels([0,2,1]))
print (result)
                      Favloc
Pet     cnts Name           
Cat     3    Kitty     couch
             Harry   windows
             Bear        bed
             Sam      basket
Dog     3    Max       floor
             Hunter   carper
Hamster 1    Fluffy  Haybale

Index.map
idx = df.index.get_level_values('Pet').map(s1.rename('cnts').get)
result = df.set_index(idx, append=True).reorder_levels([0,2,1])
print (result)
                      Favloc
Pet     cnts Name           
Cat     3    Kitty     couch
             Harry   windows
             Bear        bed
             Sam      basket
Dog     3    Max       floor
             Hunter   carper
Hamster 1    Fluffy  Haybale
",
"You're using the csv module, but ignoring csv.reader.  It handles all the parsing for you:Input:Output in the .csv file:EDIT: I see now that you want the data in Excel to be in a single cell.  The above will put it in a row of four cells:The following will write a single cell:Output in the .csv file:Output in Excel:",https://i.stack.imgur.com/gVpaq.png https://i.stack.imgur.com/vi59a.png,"csv
csv.reader
#!python2
import csv
with open('book2.csv','rb') as inf, open('test.csv','wb') as outf:
    r = csv.reader(inf)
    w = csv.writer(outf)
    L = [row for row in r] # Read all lines as list of lists.
    L = zip(*L)            # transpose all the lines.
    w.writerows(L)         # Write them all back out.

222
333
444
555

222,333,444,555

#!python2
import csv
with open('book2.csv') as inf, open('test.csv','wb') as outf:
    w = csv.writer(outf)
    data = inf.read().splitlines()
    w.writerow([','.join(data)])

""222,333,444,555""
",
This should do it assuming your columns are space separated and the empty elements are zero,,"with open('file1') as file1, open('file2') as file2,\
        open('file3') as file3, open('output', 'w') as out_file:

    lf1 = in_file1.readline()
    lf2 = in_file2.readline()
    lf3 = in_file3.readline()

    line_f1 = lf1.strip() if lf1 else "" 0 ""
    line_f2 = lf2.strip() if lf2 else "" 0 ""
    line_f3 = lf3.strip() if lf3 else "" 0 ""

    while (len(lf1)!=0 and len(lf2)!=0 and len(lf3)!=0):

            print >> outfile line_f1, line_f2, line_f3

            lf1 = in_file1.readline()
            lf2 = in_file2.readline()
            lf3 = in_file3.readline()

            line_f1 = lf1.strip() if lf1 else "" 0 ""
            line_f2 = lf2.strip() if lf2 else "" 0 ""
            line_f3 = lf3.strip() if lf3 else "" 0 ""
",
Below code do what you need. I assume each element is separated by white space. This code works with irregular column length and there is no need to put 0 for empty elements in shorter columns. I tested it with simple examples.,,"def read_file(name):
    rows = open(name, 'r').readlines()
    data = []
    for r in rows:
        data.append(r.split())
    return data

def invert(data):
    new_data = []
    max_col = 0
    for r in data:
        if len(r) > max_col:
            max_col = len(r)
    col_no = max_col
    for i in range(col_no):
        cal = []
        for j in range(len(data)):
            if i in range(len(data[j])):
                cal.append(data[j][i])
            else:
                cal.append('   ')
        new_data.append(cal)
    return new_data

def merge(data_list):
    big_data = []
    for d in data_list:
        for c in d:
            big_data.append(c)
    return big_data

def write_result(result):
    result_file = open('result', 'w')
    for r in result:
        for e in r:
            result_file.write(e + ' ')
        result_file.write('\n')


data1 = read_file('file1')
data2 = read_file('file2')
data3 = read_file('file3')
new_data1 = invert(data1)
new_data2 = invert(data2)
new_data3 = invert(data3)
big_data = merge([new_data1, new_data2, new_data3])
result = invert(big_data)
write_result(result)
",
"First of all, I assumed you've not repeated your strings correctly (like ""hi, this is an example line."" != ""hi, this is edited line."") by mistake, not on purpose (that I can't figure out).I named the accumulative file common.doc to distinct from the other .txt files in the target directory. Also, this example code implies all the files are in the same directory.And after common.doc editing:And a solution for multiline text (merging stays with .strip() removed on content writing), not suitable for hundreds of thousands of files tho...",,"common.doc
.txt
# merging.py
import os
import glob

with open(""common.doc"", ""w"") as common:
    for txt in glob.glob(""./*.txt""):
        with open(txt, ""r"") as f:
            content = f.read()
        common.write(""{} ({})\n"".format(os.path.basename(txt), content))

common.doc
# splitting.py
with open(""common.doc"", ""r"") as common:
    for line in common:
        name = line[:line.find("" ("")]
        text = line[line.find("" ("")+2:line.rfind("")"")]
        with open(name, ""w"") as f:
            f.write(text)

.strip()
# splitting2.py
with open(""common.doc"", ""r"") as common:
    everything = common.read()
elements = everything.split("")"")
for elem in elements:
    name = elem[:elem.find("" ("")].strip()
    text = elem[elem.find("" ("")+2:]
    if name:
        with open(name, ""w"") as f:
            f.write(text)
",
"You could go ahead and do the concatenation inside your for loop. However, if you are set on doing the concatenation after the fact and want them separated by commas, then I assume you are okay with the data becoming strings instead of floats. If that is the case, and you know that the columns and indexes are identical and in the same order, you can do: ",,"df = hapX_count.astype(str) + ',' + hapY_count.astype(str)
",
"Using pandas:To get rid of duplicate rows as well:This will not get rid of duplicates as the dataframe is created, but after. So a dataframe gets created by concatenating all of the files. Then it is de-duplicated. The final dataframe can then be saved to csv.",,"import pandas as pd

interesting_files = glob.glob(""/home/tcs/PYTHONMAP/test1/*.csv"") 
df = pd.concat((pd.read_csv(f, header = 0) for f in interesting_files))
df.to_csv(""output.csv"")

import pandas as pd

interesting_files = glob.glob(""/home/tcs/PYTHONMAP/test1/*.csv"") 
df = pd.concat((pd.read_csv(f, header = 0) for f in interesting_files))
df_deduplicated = df.drop_duplicates()
df_deduplicated.to_csv(""output.csv"")
",
"With python's csv reader and writer,
reading from the files a.csv and b.csv, writing to c.csv:Creates file c.csv:(I assume that those braces and extra linebreaks in your csv code are just here on stackoverflow and not really part of your csv files? I also assume that a.csv and b.csv have the same length in lines.)",,"a.csv
b.csv
c.csv
# -*- coding: utf-8 -*-

import csv

with open('a.csv', 'r') as file_a:
    with open('b.csv', 'r') as file_b:
        with open('c.csv', 'w') as file_c:
            reader_a = csv.reader(file_a, delimiter=',')
            reader_b = csv.reader(file_b, delimiter=',')
            writer_c = csv.writer(file_c)

            for cols_a in reader_a:
                cols_b = reader_b.next()
                writer_c.writerow(cols_a + cols_b)

c.csv
a, b, c, d,o, p, q, r
e, f, g, h,s, t, u, v
i, j, k, l,w, x, y, z
",
"A little bit tricky , using shift create the groupkey , then agg ",,"shift
agg
df.fillna('NaN',inplace=True) # notice here NaN always no equal to NaN, so I replace it with string 'NaN'
df.groupby((df.drop('Y',1)!=df.drop('Y',1).shift()).any(1).cumsum()).\
     agg(lambda x : ','.join(x) if x.name=='Y' else x.iloc[0])
Out[19]: 
         Y   X1    X2    X3    X4    X5
1    A,B,C  NaN -3810  TRUE  None  None
2  D,E,F,G  NaN -3810  None  None  None
3        H  NaN -3810  TRUE  None  None
",
Good to remember that itertools.groupby handles constructiveness for us.,,"itertools.groupby
itertools.groupby
from itertools import groupby

Y = df.Y
X = df.filter(like='X').T  # df.drop('Y', 1).T
K = lambda x: (*X[x].fillna('NA'),)

tups = [
    (' '.join(Y.loc[V]), *X[V[0]])
    for _, [*V] in groupby(Y.index, key=K)
]

pd.DataFrame(tups, columns=df.columns)

         Y  X1    X2    X3    X4    X5
0    A B C NaN -3810  TRUE  None  None
1  D E F G NaN -3810  None  None  None
2        H NaN -3810  TRUE  None  None
3        I NaN  2540  TRUE  None  None
4        J NaN  2540  None  True  None
",
