{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import nan\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_que = pd.read_csv('data/relevant_questions.csv')\n",
    "ans = pd.read_csv('data/keyword_answer.csv')\n",
    "kdf = pd.read_csv('data/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/original_data1.csv')\n",
    "df2 = pd.read_csv('data/original_data2.csv')\n",
    "df3 = pd.read_csv('data/original_data3.csv')\n",
    "df4 = pd.read_csv('data/original_data4.csv')\n",
    "df5 = pd.read_csv('data/original_data5.csv')\n",
    "df = pd.concat([df1,df2,df3,df4,df5], axis=0)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "from bs4 import BeautifulSoup\n",
    "df.drop(columns=['answers', 'tags'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic0_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dcbd6868ed4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ans length'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'answer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmax_topic0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic0_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ans length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmin_topic0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic0_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ans length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnormalised_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topic0_df' is not defined"
     ]
    }
   ],
   "source": [
    "def length_text(text):\n",
    "    if(type(text) != type(0.0)):\n",
    "        text = text.split(' ')\n",
    "        return len(text)\n",
    "    else:\n",
    "        return 0\n",
    "ans['ans length'] = ans['answer'].apply(length_text)\n",
    "\n",
    "max_topic0 = topic0_df['ans length'].max()\n",
    "min_topic0 = topic0_df['ans length'].min()\n",
    "def normalised_length(length):\n",
    "    nor_len = ((length - min_topic0) / (max_topic0 - min_topic0))\n",
    "    return nor_len\n",
    "topic0_df['ans length'] = topic0_df['ans length'].apply(normalised_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ans = pd.DataFrame()\n",
    "for i in enumerate(rel_que['id']):\n",
    "    rel_ans = pd.concat([ans[ans['id'] == i[1]], rel_ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(topic_list):\n",
    "    if(type(topic_list) == type(0.0)):\n",
    "        return pd.Series([0,0,0,0,0,0,0,0])\n",
    "    else:\n",
    "        topic_list = topic_list.split(', ')\n",
    "        tl = []\n",
    "        for topic in topics:\n",
    "            if(topic in topic_list):\n",
    "                val = 1\n",
    "            else: \n",
    "                val = 0\n",
    "            tl.append(val)\n",
    "        return pd.Series(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ans[topics] = rel_ans['topic list'].apply(get_dataframe)\n",
    "rel_ans.drop(columns=['topic list', 'id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_ans = pd.DataFrame()\n",
    "for i in enumerate(rel_que['id']):\n",
    "    que_ans = pd.concat([df[df['id'] == i[1]], que_ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    txt = \"\".join([txt.text for txt in soup.find_all(\"p\")])\n",
    "    return txt\n",
    "que_ans['body'] = que_ans['body'].apply(extract_text)\n",
    "que_ans['question'] = que_ans['title'] + que_ans['body']\n",
    "que_ans.drop(columns=['title', 'body'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    if(type(text) != type(0.0)):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r'[0-9]', ' ', text)\n",
    "        text = text.strip()\n",
    "        token = word_tokenize(text)\n",
    "        text = [i for i in token if not i in stop_words]\n",
    "        output = []\n",
    "        for word in text:\n",
    "            output.append(stemmer.stem(word))\n",
    "        text = []\n",
    "        for word in output:\n",
    "            text.append(lemmatizer.lemmatize(word))\n",
    "        text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_ans['question list'] = que_ans['question'].apply(text_process)\n",
    "rel_ans['answer list'] = rel_ans['answer'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rel_ans.join(que_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(question, answer):\n",
    "    sentences = [question, answer]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vector = vectorizer.fit_transform(sentences)\n",
    "    text1 = vector.toarray()[0].tolist()\n",
    "    text2 = vector.toarray()[1].tolist()\n",
    "    cosc = 1-distance.cosine(text1, text2)\n",
    "    return cosc\n",
    "    \n",
    "def entropy(text):\n",
    "    if(type(text) == type(0.0)):\n",
    "        return 0\n",
    "    else:\n",
    "        token = word_tokenize(text)\n",
    "        entropy = 0\n",
    "        for word in text:\n",
    "            try:\n",
    "                entropy = entropy + tfidf_array[word]\n",
    "            except:\n",
    "                entropy = entropy + 0\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'merge two lists in python'\n",
    "query = text_process(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data['answer list'].dropna().tolist()\n",
    "cv = CountVectorizer(stop_words = stop_words)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit_transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_transformer.idf_\n",
    "words = cv.get_feature_names()\n",
    "tfidf_dict = {}\n",
    "for i in range(0, len(tfidf_transformer.idf_)):\n",
    "    tfidf_dict[tfidf_array[i]] = words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_df = data[data['Topic 0']==1][['id', 'answer', 'link', 'code', 'ans length', 'question list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_score(score, min_score, max_score):\n",
    "    score = ((score - min_score)/(max_score - min_score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    topic_data = data[data[topic]==1][['id', 'answer', 'link', 'code', 'score', 'answer list', 'question list']]\n",
    "    entropy_score = []\n",
    "    topic_df = pd.DataFrame()\n",
    "    for i in range(0, topic_data.shape[0]):\n",
    "        cosc = cosine_score(topic_data.iloc[i]['question list'], topic_data.iloc[i]['answer list'])\n",
    "        topic_df = topic_df.append(pd.Series([topic_data.iloc[i]['answer'], topic_data.iloc[i]['link'], topic_data.iloc[i]['code'], topic_data.iloc[i]['score'], cosc]), ignore_index=True)\n",
    "    topic_df.columns = ['answer', 'link', 'code', 'score', 'cosine score']\n",
    "    topic_df['entropy'] = data['answer list'].apply(entropy)\n",
    "    min_user_score = topic_df['score'].min()\n",
    "    max_user_score = topic_df['score'].max()\n",
    "    min_cosine_score = topic_df['cosine score'].min()\n",
    "    max_cosine_score = topic_df['cosine score'].max()\n",
    "    min_entropy_score = topic_df['entropy'].min()\n",
    "    max_entropy_score = topic_df['entropy'].max()\n",
    "    for i in range(0, topic_df.shape[0]):\n",
    "        topic_df.iloc[i]['score'] = normalised_score(topic_df.iloc[i]['score'], min_user_score, max_user_score)\n",
    "        topic_df.iloc[i]['cosine score'] = normalised_score(topic_df.iloc[i]['cosine score'], min_cosine_score, max_cosine_score)\n",
    "        topic_df.iloc[i]['entropy'] = normalised_score(topic_df.iloc[i]['entropy'], min_entropy_score, max_entropy_score)\n",
    "    topic_df['sc'] = topic_df['score'] + topic_df['cosine score'] + topic_df['entropy']\n",
    "    topic_df.sort_values(by = 'sc', ascending=False, inplace=True, kind='quicksort')\n",
    "    topic_df.drop(columns = ['score', 'cosine score', 'entropy'], inplace=True)\n",
    "    path = 'data/'+ topic +'.csv'\n",
    "    topic_df = topic_df.head(15)\n",
    "    topic_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
